{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14604407,"sourceType":"datasetVersion","datasetId":9328614},{"sourceId":14604450,"sourceType":"datasetVersion","datasetId":9328642}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"acd28af9","cell_type":"markdown","source":"## Install the repo","metadata":{}},{"id":"67f246f7","cell_type":"code","source":"%rm -rf Visual-Place-Recognition-Project\n!git clone -b adaptive_reranking --single-branch --recursive https://github.com/Digre01/Visual-Place-Recognition-Project.git\n","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true},"outputs":[],"execution_count":null},{"id":"1227c2ed","cell_type":"code","source":"%cd Visual-Place-Recognition-Project/image-matching-models\n!pip install -e .[all]","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true},"outputs":[],"execution_count":null},{"id":"4481496d","cell_type":"code","source":"!pip install faiss-cpu\n!pip install yacs\n!pip install loguru","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true},"outputs":[],"execution_count":null},{"id":"af670f37","cell_type":"markdown","source":"## Download datasets","metadata":{}},{"id":"91d92f36-9db1-456a-89f0-db846bae91f1","cell_type":"code","source":"!rm -rf \"/kaggle/working/Visual-Place-Recognition-Project/data\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"76ca2797-d1b0-43da-bc65-c60a3a376130","cell_type":"code","source":"%ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T14:26:24.104204Z","iopub.execute_input":"2026-01-24T14:26:24.105025Z","iopub.status.idle":"2026-01-24T14:26:24.232754Z","shell.execute_reply.started":"2026-01-24T14:26:24.104972Z","shell.execute_reply":"2026-01-24T14:26:24.231599Z"}},"outputs":[{"name":"stdout","text":"adaptive_reranking.py\n\u001b[0m\u001b[01;34mdata\u001b[0m/\ndataset_cosplace_loftr.csv\ndataset_cosplace_loftr_validation.csv\ndataset_cosplace_superpoint-lg.csv\ndataset_cosplace_superpoint-lg_validation.csv\ndataset_netvlad_loftr.csv\ndataset_netvlad_loftr_validation.csv\ndataset_netvlad_superpoint-lg.csv\ndataset_netvlad_superpoint-lg_validation.csv\ndataset_val.csv\ndownload_datasets.py\n\u001b[01;34mimage-matching-models\u001b[0m/\n\u001b[01;34minliers_full_cosplace_loftr\u001b[0m/\n\u001b[01;34minliers_full_cosplace_superpoint-lg\u001b[0m/\n\u001b[01;34minliers_full_netvlad_loftr\u001b[0m/\n\u001b[01;34minliers_full_netvlad_superpoint-lg\u001b[0m/\nLICENSE\nlogistic_comparison.png\nlogistic_regression.ipynb\n\u001b[01;34mlogs\u001b[0m/\nmatch_queries_preds.py\nmodel_cosplace_loftr.joblib\nmodel_cosplace_loftr_validation.joblib\nmodel_cosplace_superpoint-lg.joblib\nmodel_netvlad_loftr.joblib\nmodel_netvlad_superpoint-lg.joblib\nprepare_logistic_data.py\n\u001b[01;34m__pycache__\u001b[0m/\nREADME.md\nreranking.py\nstart_your_project.ipynb\nstate.db\n\u001b[01;34mtrained_models\u001b[0m/\ntrain_logReg.py\nutil.py\n\u001b[01;34mVPR-methods-evaluation\u001b[0m/\n\u001b[01;34mvpr_uncertainty\u001b[0m/\n","output_type":"stream"}],"execution_count":60},{"id":"dbc0d30b-51d0-49da-887c-3ea2d5d57f6d","cell_type":"code","source":"%cd Visual-Place-Recognition-Project/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ff37cc2c-9800-4fe9-9953-0fc8e6712923","cell_type":"code","source":"%%writefile download_datasets.py\n\nURLS = {\n    \"tokyo_xs\": \"https://drive.google.com/file/d/15QB3VNKj93027UAQWv7pzFQO1JDCdZj2/view?usp=share_link\",\n    \"sf_xs\": \"https://drive.google.com/file/d/1tQqEyt3go3vMh4fj_LZrRcahoTbzzH-y/view?usp=share_link\",\n    #\"gsv_xs\": \"https://drive.google.com/file/d/1q7usSe9_5xV5zTfN-1In4DlmF5ReyU_A/view?usp=share_link\",\n    \"svox\": \"https://drive.google.com/file/d/16iuk8voW65GaywNUQlWAbDt6HZzAJ_t9/view?usp=drive_link\"\n} \n\nimport os\nimport gdown\nimport shutil\n\nos.makedirs(\"data\", exist_ok=True)\nfor dataset_name, url in URLS.items():\n    print(f\"Downloading {dataset_name}\")\n    zip_filepath = f\"data/{dataset_name}.zip\"\n    gdown.download(url, zip_filepath, fuzzy=True)\n    shutil.unpack_archive(zip_filepath, extract_dir=\"data\")\n    os.remove(zip_filepath)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:39:59.582261Z","iopub.execute_input":"2026-01-24T16:39:59.583103Z","iopub.status.idle":"2026-01-24T16:39:59.589609Z","shell.execute_reply.started":"2026-01-24T16:39:59.583048Z","shell.execute_reply":"2026-01-24T16:39:59.588723Z"}},"outputs":[{"name":"stdout","text":"Overwriting download_datasets.py\n","output_type":"stream"}],"execution_count":95},{"id":"6a3092b2-ea39-4688-aa1f-553e23a8a378","cell_type":"code","source":"%rm -rf /kaggle/working/Visual-Place-Recognition-Project/logs\n%rm -rf /kaggle/working/Visual-Place-Recognition-Project/data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:43:30.213694Z","iopub.execute_input":"2026-01-24T16:43:30.214670Z","iopub.status.idle":"2026-01-24T16:43:35.746216Z","shell.execute_reply.started":"2026-01-24T16:43:30.214625Z","shell.execute_reply":"2026-01-24T16:43:35.745264Z"}},"outputs":[],"execution_count":98},{"id":"e9f554b2","cell_type":"code","source":"!python download_datasets.py","metadata":{"vscode":{"languageId":"plaintext"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:43:38.670570Z","iopub.execute_input":"2026-01-24T16:43:38.670926Z","iopub.status.idle":"2026-01-24T16:45:39.924492Z","shell.execute_reply.started":"2026-01-24T16:43:38.670890Z","shell.execute_reply":"2026-01-24T16:45:39.923372Z"}},"outputs":[{"name":"stdout","text":"Downloading tokyo_xs\nDownloading...\nFrom (original): https://drive.google.com/uc?id=15QB3VNKj93027UAQWv7pzFQO1JDCdZj2\nFrom (redirected): https://drive.google.com/uc?id=15QB3VNKj93027UAQWv7pzFQO1JDCdZj2&confirm=t&uuid=ab784cd4-3aaa-4404-ac0d-78c03b05ea2c\nTo: /kaggle/working/Visual-Place-Recognition-Project/data/tokyo_xs.zip\n100%|████████████████████████████████████████| 141M/141M [00:01<00:00, 86.3MB/s]\nDownloading sf_xs\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1tQqEyt3go3vMh4fj_LZrRcahoTbzzH-y\nFrom (redirected): https://drive.google.com/uc?id=1tQqEyt3go3vMh4fj_LZrRcahoTbzzH-y&confirm=t&uuid=e503070a-055a-4832-99c5-df286313ef75\nTo: /kaggle/working/Visual-Place-Recognition-Project/data/sf_xs.zip\n100%|██████████████████████████████████████| 1.03G/1.03G [00:14<00:00, 70.2MB/s]\nDownloading svox\nDownloading...\nFrom (original): https://drive.google.com/uc?id=16iuk8voW65GaywNUQlWAbDt6HZzAJ_t9\nFrom (redirected): https://drive.google.com/uc?id=16iuk8voW65GaywNUQlWAbDt6HZzAJ_t9&confirm=t&uuid=e92dc69a-d994-479e-b98f-8fcb7e82d361\nTo: /kaggle/working/Visual-Place-Recognition-Project/data/svox.zip\n100%|██████████████████████████████████████| 3.51G/3.51G [00:46<00:00, 75.5MB/s]\n","output_type":"stream"}],"execution_count":99},{"id":"fabae9d8","cell_type":"markdown","source":"## Pipeline","metadata":{}},{"id":"25c3df35-b5fb-4007-b072-08734c2ff286","cell_type":"code","source":"%rm dataset_cosplace_loftr.csv\n%rm dataset_cosplace_superpoint-lg.csv\n%rm dataset_netvlad_loftr.csv\n%rm dataset_netvlad_superpoint-lg.csv\n%rm -rf inliers_full_cosplace_loftr\n%rm -rf inliers_full_cosplace_superpoint-lg\n%rm -rf inliers_full_netvlad_loftr\n%rm -rf inliers_full_netvlad_superpoint-lg\n%rm -rf \"/kaggle/working/Visual-Place-Recognition-Project/model_cosplace_loftr.joblib\"\n%rm -rf \"/kaggle/working/Visual-Place-Recognition-Project/model_cosplace_superpoint-lg.joblib\"\n%rm -rf \"/kaggle/working/Visual-Place-Recognition-Project/model_netvlad_loftr.joblib\"\n%rm -rf \"/kaggle/working/Visual-Place-Recognition-Project/model_netvlad_superpoint-lg.joblib\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T15:28:27.484220Z","iopub.execute_input":"2026-01-24T15:28:27.484590Z","iopub.status.idle":"2026-01-24T15:28:28.958337Z","shell.execute_reply.started":"2026-01-24T15:28:27.484555Z","shell.execute_reply":"2026-01-24T15:28:28.957443Z"}},"outputs":[{"name":"stdout","text":"rm: cannot remove 'dataset_cosplace_loftr.csv': No such file or directory\nrm: cannot remove 'dataset_cosplace_superpoint-lg.csv': No such file or directory\nrm: cannot remove 'dataset_netvlad_loftr.csv': No such file or directory\nrm: cannot remove 'dataset_netvlad_superpoint-lg.csv': No such file or directory\n","output_type":"stream"}],"execution_count":79},{"id":"fae942c5-08cd-4cf5-885b-3f3932823546","cell_type":"code","source":"%ls \nprint(gsv_db, gsv_queries)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0541b6a5","cell_type":"code","source":"import os\nimport shutil\n\nCONFIGS = [\n    {\"vpr\": \"cosplace\", \"backbone\": \"ResNet18\", \"dim\": 512, \"matcher\": \"loftr\"},\n    {\"vpr\": \"cosplace\", \"backbone\": \"ResNet18\", \"dim\": 512, \"matcher\": \"superpoint-lg\"},\n    {\"vpr\": \"netvlad\", \"backbone\": \"VGG16\", \"dim\": 4096, \"matcher\": \"loftr\"},\n    {\"vpr\": \"netvlad\", \"backbone\": \"VGG16\", \"dim\": 4096, \"matcher\": \"superpoint-lg\"}\n]\n\n\n\nfor conf in CONFIGS:\n    v_name = conf[\"vpr\"]\n    m_name = conf[\"matcher\"]\n    log_tag = f\"train_{v_name}\"\n    \n    print(f\"\\n{'='*60}\\Running: {v_name.upper()} + {m_name.upper()}\\n{'='*60}\")\n    \n    #RETRIVAL\n    !python VPR-methods-evaluation/main.py \\\n        --method={v_name} --backbone={conf['backbone']} --descriptors_dimension={conf['dim']} \\\n        --num_workers 4 --batch_size 32 \\\n        --log_dir {log_tag} \\\n        --image_size 512 512 \\\n        --database_folder 'data/svox/images/train/gallery' \\\n        --queries_folder 'data/svox/images/train/queries_sun' \\\n        --recall_values 1 5 10 20 --num_preds_to_save 20\n    \n\n    base_log = f\"logs/{log_tag}\"\n    latest_run = sorted(os.listdir(base_log))[-1]\n    preds_dir = os.path.join(base_log, latest_run, \"preds\")\n    print(preds_dir)\n    \n\n    #MATCHING\n    out_inliers = f\"inliers_full_{v_name}_{m_name}\"\n    !python match_queries_preds.py \\\n        --preds-dir {preds_dir} \\\n        --out-dir {out_inliers} \\\n        --matcher {m_name} \\\n        --num-preds 20\n    \n    #TRAINING\n    csv_file = f\"dataset_{v_name}_{m_name}.csv\"\n    model_file = f\"model_{v_name}_{m_name}.joblib\"\n    \n    !python prepare_logistic_data.py --preds-dir {preds_dir} --inliers-dir {out_inliers} --output-csv {csv_file}\n    !python train_logReg.py --csv-path {csv_file} --model-out {model_file}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f745ea5a","cell_type":"markdown","source":"## Visualization","metadata":{}},{"id":"b0f5dcd7","cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport joblib\nimport glob\n\nCONFIGS = [\n    {\"vpr\": \"cosplace\", \"backbone\": \"ResNet18\", \"dim\": 512, \"matcher\": \"loftr\"},\n    {\"vpr\": \"cosplace\", \"backbone\": \"ResNet18\", \"dim\": 512, \"matcher\": \"superpoint-lg\"},\n    {\"vpr\": \"netvlad\", \"backbone\": \"VGG16\", \"dim\": 4096, \"matcher\": \"loftr\"},\n    {\"vpr\": \"netvlad\", \"backbone\": \"VGG16\", \"dim\": 4096, \"matcher\": \"superpoint-lg\"}\n]\n\nplt.figure(figsize=(12, 7))\n\nx_test = np.linspace(0, 100, 500).reshape(-1, 1)\n\ncolors = ['blue', 'cyan', 'red', 'orange']\nstyles = ['-', '--', '-', '--']\n\nfor i, conf in enumerate(CONFIGS):\n    m_path = f\"model_{conf['vpr']}_{conf['matcher']}.joblib\"\n    if os.path.exists(m_path):\n        model = joblib.load(m_path)\n        # Calcola la probabilità P(y=1 | x)\n        probs = model.predict_proba(x_test)[:, 1]\n        \n        label = f\"{conf['vpr']} + {conf['matcher']}\"\n        plt.plot(x_test, probs, label=label, color=colors[i], linestyle=styles[i], lw=2)\n\nplt.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5)\nplt.title(\"Probability correct match\", fontsize=14)\nplt.xlabel(\"Num inliers (Top-1)\", fontsize=12)\nplt.ylabel(\"Probability (distance < 25m)\", fontsize=12)\nplt.grid(True, which='both', linestyle='--', alpha=0.4)\nplt.legend(loc='lower right', fontsize=10)\nplt.xlim(0, 100) \nplt.ylim(0, 1.05)\n\nplt.savefig(\"logistic_comparison.png\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"da312dd2-5408-4463-8bd0-45a7a094d325","cell_type":"markdown","source":"## Validation","metadata":{}},{"id":"daa8a06c-4b4e-4590-8deb-573fb1bee42d","cell_type":"code","source":"import os\nimport shutil\n\nCONFIGS = [\n    {\"vpr\": \"cosplace\", \"backbone\": \"ResNet18\", \"dim\": 512, \"matcher\": \"loftr\"},\n    {\"vpr\": \"cosplace\", \"backbone\": \"ResNet18\", \"dim\": 512, \"matcher\": \"superpoint-lg\"},\n    {\"vpr\": \"netvlad\", \"backbone\": \"VGG16\", \"dim\": 4096, \"matcher\": \"loftr\"},\n    {\"vpr\": \"netvlad\", \"backbone\": \"VGG16\", \"dim\": 4096, \"matcher\": \"superpoint-lg\"}\n]\n\n\n\nfor conf in CONFIGS:\n    v_name = conf[\"vpr\"]\n    m_name = conf[\"matcher\"]\n    log_tag = f\"train_{v_name}\"\n    \n    print(f\"\\n{'='*60} Running: {v_name.upper()} + {m_name.upper()}\\n{'='*60}\")\n    \n    !python VPR-methods-evaluation/main.py \\\n        --method={v_name} --backbone={conf['backbone']} --descriptors_dimension={conf['dim']} \\\n        --num_workers 4 --batch_size 32 \\\n        --log_dir {log_tag} \\\n        --image_size 512 512 \\\n        --database_folder 'data/sf_xs/val/database' \\\n        --queries_folder 'data/sf_xs/val/queries' \\\n        --recall_values 1 5 10 20 --num_preds_to_save 20\n    \n\n    base_log = f\"logs/{log_tag}\"\n    latest_run = sorted(os.listdir(base_log))[-1]\n    preds_dir = os.path.join(base_log, latest_run, \"preds\")\n    print(preds_dir)\n    \n    out_inliers = f\"inliers_full_{v_name}_{m_name}\"\n    !python match_queries_preds.py \\\n        --preds-dir {preds_dir} \\\n        --out-dir {out_inliers} \\\n        --matcher {m_name} \\\n        --num-preds 20\n    \n    #TRAINING\n    csv_file = f\"dataset_{v_name}_{m_name}_validation.csv\"\n    model_file = f\"model_{v_name}_{m_name}_validation.joblib\"\n    \n    !python prepare_logistic_data.py --preds-dir {preds_dir} --inliers-dir {out_inliers} --output-csv {csv_file}\n    !python train_logReg.py --csv-path {csv_file} --model-out {model_file}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9e85cf5e-c8a4-4483-9d6a-142836d067d7","cell_type":"code","source":"!python train_logReg.py --csv-path \"/kaggle/input/cosplace-loftr/dataset_cosplace_loftr.csv\" --model-out \"model_cosplace_loftr.joblib\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T14:19:01.385808Z","iopub.execute_input":"2026-01-24T14:19:01.386138Z","iopub.status.idle":"2026-01-24T14:19:03.066497Z","shell.execute_reply.started":"2026-01-24T14:19:01.386111Z","shell.execute_reply":"2026-01-24T14:19:03.065364Z"}},"outputs":[{"name":"stdout","text":"Model saved as model_cosplace_loftr.joblib\n","output_type":"stream"}],"execution_count":55},{"id":"21dbcdc2-58c8-4ea5-848e-4d28585dca66","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport joblib\nfrom sklearn.metrics import precision_score\n\n# Validation parameters\nthresholds = np.linspace(0.1, 0.95, 18)\ntarget_precision = 0.99  #Minimum Precision for TOP-1\n\n\nv_name, m_name = conf[\"vpr\"], conf[\"matcher\"]\nmodel_file =  \"model_cosplace_loftr.joblib\"\n\n#there is only SF_XS val\nval_csv = \"/kaggle/input/cosplace-loftr-validation/dataset_cosplace_loftr_validation.csv\"\n\ntry:\n    model = joblib.load(model_file)\n    df = pd.read_csv(val_csv)\n\n    \n    probs = model.predict_proba(df[['num_inliers']].values)[:, 1]\n    \n    best_t, max_savings = 0.5, 0.0\n    \n    for t in thresholds:\n        preds = (probs >= t).astype(int)\n        precision = precision_score(df['label'], preds, zero_division=0)\n        \n        if precision >= target_precision:\n            #1 match if stopped, 20 matches otherwise\n            avg_matches = np.mean([1 if p == 1 else 20 for p in preds])\n            savings = (1 - (avg_matches / 20)) * 100\n            if savings >= max_savings:\n                max_savings = savings\n                best_t = t\n                \n    print(f\"{v_name.upper()} + {m_name.upper()} | Selected Threshold: {best_t:.2f} | Validation Savings: {max_savings:.1f}%\")\n    \nexcept FileNotFoundError:\n    print(\"Error\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T15:05:05.225558Z","iopub.execute_input":"2026-01-24T15:05:05.225919Z","iopub.status.idle":"2026-01-24T15:05:05.307011Z","shell.execute_reply.started":"2026-01-24T15:05:05.225883Z","shell.execute_reply":"2026-01-24T15:05:05.306040Z"}},"outputs":[{"name":"stdout","text":"COSPLACE + LOFTR | Selected Threshold: 0.65 | Validation Savings: 77.3%\n","output_type":"stream"}],"execution_count":75},{"id":"34aec7f0-7e2d-4577-bb5c-f3a3b60dc625","cell_type":"markdown","source":"## Testing","metadata":{}},{"id":"8f811974-4fec-4ae1-8dcd-48132f5b6b84","cell_type":"code","source":"CONFIGS = [\n    {\"vpr\": \"cosplace\", \"backbone\": \"ResNet18\", \"dim\": 512, \"matcher\": \"loftr\"},\n    {\"vpr\": \"cosplace\", \"backbone\": \"ResNet18\", \"dim\": 512, \"matcher\": \"superpoint-lg\"},\n    {\"vpr\": \"netvlad\", \"backbone\": \"VGG16\", \"dim\": 4096, \"matcher\": \"loftr\"},\n    {\"vpr\": \"netvlad\", \"backbone\": \"VGG16\", \"dim\": 4096, \"matcher\": \"superpoint-lg\"}\n]\n\nconf = CONFIGS[0]\nlog_tag = f\"train_{conf['vpr']}\"\n\n!python VPR-methods-evaluation/main.py \\\n        --method={v_name} --backbone={conf['backbone']} --descriptors_dimension={conf['dim']} \\\n        --num_workers 4 --batch_size 32 \\\n        --log_dir {log_tag} \\\n        --image_size 512 512 \\\n        --database_folder 'data/svox/images/test/gallery' \\\n        --queries_folder 'data/svox/images/test/queries_night' \\\n        --recall_values 1 5 10 20 --num_preds_to_save 20","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T17:17:59.354655Z","iopub.execute_input":"2026-01-24T17:17:59.355038Z","iopub.status.idle":"2026-01-24T17:22:19.264185Z","shell.execute_reply.started":"2026-01-24T17:17:59.355002Z","shell.execute_reply":"2026-01-24T17:22:19.263290Z"}},"outputs":[{"name":"stdout","text":"\u001b[32m2026-01-24 17:18:04\u001b[0m VPR-methods-evaluation/main.py --method=cosplace --backbone=ResNet18 --descriptors_dimension=512 --num_workers 4 --batch_size 32 --log_dir train_cosplace --image_size 512 512 --database_folder data/svox/images/test/gallery --queries_folder data/svox/images/test/queries_night --recall_values 1 5 10 20 --num_preds_to_save 20\n\u001b[32m2026-01-24 17:18:04\u001b[0m Arguments: Namespace(positive_dist_threshold=25, method='cosplace', backbone='ResNet18', descriptors_dimension=512, faiss_metric='L2', database_folder='data/svox/images/test/gallery', queries_folder='data/svox/images/test/queries_night', num_workers=4, batch_size=32, log_dir='train_cosplace', device='cuda', recall_values=[1, 5, 10, 20], no_labels=False, num_preds_to_save=20, save_only_wrong_preds=False, image_size=[512, 512], save_descriptors=False, save_for_uncertainty=False, use_labels=True)\n\u001b[32m2026-01-24 17:18:04\u001b[0m Testing with cosplace with a ResNet18 backbone and descriptors dimension 512\n\u001b[32m2026-01-24 17:18:04\u001b[0m The outputs are being saved in logs/train_cosplace/2026-01-24_17-18-04\nUsing cache found in /root/.cache/torch/hub/gmberton_cosplace_main\nReturning CosPlace model with backbone: ResNet18 with features dimension 512\nSearching test images in data/svox/images/test/gallery with glob()\nSearching test images in data/svox/images/test/queries_night with glob()\n\u001b[32m2026-01-24 17:18:05\u001b[0m Testing on < #queries: 823; #database: 17166 >\n100%|█████████████████████████████████████████| 537/537 [01:31<00:00,  5.84it/s]\n100%|████████████████████████████████████████| 823/823 [00:07<00:00, 112.81it/s]\n/kaggle/working/Visual-Place-Recognition-Project/VPR-methods-evaluation/main.py:95: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n  if np.any(np.in1d(preds[:n], positives_per_query[query_index])):\n\u001b[32m2026-01-24 17:19:45\u001b[0m R@1: 33.3, R@5: 51.5, R@10: 59.1, R@20: 67.7\n\u001b[32m2026-01-24 17:19:45\u001b[0m Saving final predictions\nSaving preds in logs/train_cosplace/2026-01-24_17-18-04/preds: 100%|█| 823/823 [\n","output_type":"stream"}],"execution_count":105},{"id":"86de9401-4d01-4456-9632-6a37ecee24f2","cell_type":"code","source":"%rm -rf '/kaggle/working/Visual-Place-Recognition-Project/logs/train_netvlad/2026-01-24_14-26-47/preds_loftr'\n!python match_queries_preds.py \\\n--preds-dir 'logs/train_cosplace/2026-01-24_17-18-04/preds' \\\n--matcher 'loftr' \\\n--device 'cuda' \\\n--num-preds 20 \\\n--logistic-model-path \"model_cosplace_loftr.joblib\" \\\n--logistic-prob-threshold 0.65  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T17:23:59.235039Z","iopub.execute_input":"2026-01-24T17:23:59.235987Z","iopub.status.idle":"2026-01-24T17:45:09.924752Z","shell.execute_reply.started":"2026-01-24T17:23:59.235943Z","shell.execute_reply":"2026-01-24T17:45:09.923895Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/Visual-Place-Recognition-Project/image-matching-models/matching/third_party/LightGlue/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n100%|█████████████████████████████████████████| 823/823 [21:02<00:00,  1.53s/it]\nTotal time: 1262.91s\nMatch: 11805 over 16460\n","output_type":"stream"}],"execution_count":107},{"id":"339ab2d4-6bb8-48ce-8699-2c47c2bc3750","cell_type":"code","source":"!python adaptive_reranking.py  \\\n    --preds-dir '/kaggle/working/Visual-Place-Recognition-Project/logs/train_cosplace/2026-01-24_17-18-04/preds' \\\n    --inliers-dir '/kaggle/working/Visual-Place-Recognition-Project/logs/train_cosplace/2026-01-24_17-18-04/preds_loftr' \\\n    --num-preds 20 \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T17:45:20.837106Z","iopub.execute_input":"2026-01-24T17:45:20.837768Z","iopub.status.idle":"2026-01-24T17:45:24.515584Z","shell.execute_reply.started":"2026-01-24T17:45:20.837731Z","shell.execute_reply":"2026-01-24T17:45:24.514626Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"Eval over 823 query...\n100%|████████████████████████████████████████| 823/823 [00:01<00:00, 739.56it/s]\n\n=============================================\nRESULTS RE-RANKING ADAPTIVE\n=============================================\nBaseline (Retrieval-only) R@1: 33.29%\nAdaptive Matching R@1:       61.12%\nComputational cost saving:    28.28%\n---------------------------------------------\nOTHER DATA:\nRecall Adattive:\n R@1: 61.12%\n R@5: 65.37%\n R@10: 66.22%\n R@20: 67.56%\n=============================================\n","output_type":"stream"}],"execution_count":108},{"id":"75487c52-946b-415e-bacb-ee7fe922bc3d","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}